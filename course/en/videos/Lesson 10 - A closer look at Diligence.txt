In this video, we'll examine the diligence competency from the AI fluency framework. Remember that AI fluency means working with AI effectively, efficiently, ethically, and safely. While the other three competencies primarily address effectiveness and efficiency, diligence focuses mostly on the ethical and safety aspects that are just as crucial for successful AI collaboration. At its heart, diligence is about taking responsibility for your AI interactions. It's the dimension of AI fluency that ensures your use of AI systems is not only productive but also rigorous, transparent, and accountable. Unlike the other competencies that primarily focus on getting results, diligence asks us to consider broader questions that are nevertheless critical to AI collaboration, particularly in professional environments, such as what are the implications of working with this AI? Who might be affected by what is created or by the collaboration itself or by any missed inaccuracies? Who has access to the data used to produce this output? How do I ensure that my interaction and the outcome aligns with ethical standards and values? Think about it like driving a car. We don't just focus on getting from point A to point B efficiently. We also consider safety, follow traffic rules, and remain aware of how our driving affects others on the road. Similarly, diligence recognizes that AI systems and our interactions with them don't exist in a vacuum. Working with AI responsibly requires awareness of broader contexts and their implications. Diligence begins with becoming more critically thoughtful about which AI systems we work with, how we work with them, and the impacts that come from those collaborations. We should seek answers to questions like, how is this system trained and built? What data was used? Who owns the data I'm inputting right now? Who may have access to it once it's shared? How am I protecting the privacy and security of myself and others? What other impacts does this system have? How does this interaction align with my personal and professional values or with my organization's policies? For example, before sharing sensitive company information with an AI assistant, it's important to first check whether the service has appropriate data protection policies in place or if your organization permits such sharing. We call this type of diligence creation diligence. It is your ability to be critical and intentional about which AI systems you choose to work with and how you work with them. Different settings, personal, academic, creative, and professional may have different expectations of disclosure about AI interaction. However, the responsibility is on each of us to understand and meet these expectations. Ask yourself, who needs to know about AI's role in this work? How and when should I communicate this? What level of detail makes sense to share? Meeting expectations for transparency, in other words, being forthright and honest, isn't just about following rules and regulations. It's about maintaining trust and respect in your relationships. It acknowledges that people have the right to know when AI has played a significant role in content creation or in decisions that affect them. For instance, if you used AI to help draft a team proposal, letting your colleagues know which parts were AI assisted allows for a more honest collaboration and keeps everyone on the same page. We call this transparency diligence. It's the ability to be open and accurate about AI interaction with everyone who needs to know. As we discussed before, AI systems can make mistakes. When you share AI generated content with the world, you not the AI are ultimately responsible for its accuracy and appropriateness. This means verifying facts, checking for biases, ensuring accuracy and usage rights, and other checks needed so that you can stand behind what you share. Consider a journalist who uses AI to help draft an article. Before publishing, they would need to verify every fact and source. Ensure that the final piece meets every journalistic standard, the same standards that would apply had they written it entirely themselves. We call this deployment diligence. It's the ability to take informed responsibility for the outputs that you use or share after they've been created with AI assistance. Navigating these diligence considerations isn't always straightforward. Different contexts and stakeholders may have different expectations and standards. So it helps to develop personal guidelines for working with AI that align with your own ethics and values. And in professional contexts, familiarize yourself with organizational policies and industry standards. And remember that the legal and regulatory frameworks around AI are still emerging and will continue to evolve. Staying informed is an important part of diligence. To recap, creation, transparency, and deployment diligence work together to form the complete diligence competency. By developing your capacity for diligence, you ensure that your AI use is not only effective and efficient, but also ethical and safe. Diligence reminds us that our interaction with AI comes with responsibilities. To be thoughtful about the systems we choose and about how we work with them, to be honest about AI's role in our work, and ultimately to be accountable for what we create when working with AI. We all want AI that is fair and safe and of benefit to our society. Our own behaviors play a key role in making this happen. 
